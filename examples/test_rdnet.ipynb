{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylerkeller/micromamba/envs/rubiks/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/tylerkeller/micromamba/envs/rubiks/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/var/folders/b0/c5vf3vjs0_q21zn670yj76_h0000gn/T/ipykernel_3061/3804000091.py:346: UserWarning: Overwriting rdnet_tiny in registry with __main__.rdnet_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/var/folders/b0/c5vf3vjs0_q21zn670yj76_h0000gn/T/ipykernel_3061/3804000091.py:361: UserWarning: Overwriting rdnet_small in registry with __main__.rdnet_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/var/folders/b0/c5vf3vjs0_q21zn670yj76_h0000gn/T/ipykernel_3061/3804000091.py:376: UserWarning: Overwriting rdnet_base in registry with __main__.rdnet_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/var/folders/b0/c5vf3vjs0_q21zn670yj76_h0000gn/T/ipykernel_3061/3804000091.py:391: UserWarning: Overwriting rdnet_large in registry with __main__.rdnet_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RDNet\n",
    "Copyright (c) 2024-present NAVER Cloud Corp.\n",
    "Apache-2.0\n",
    "\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers.squeeze_excite import EffectiveSEModule\n",
    "from timm.models import register_model, build_model_with_cfg, named_apply, generate_default_cfgs\n",
    "from timm.models.layers import DropPath\n",
    "from timm.models.layers import LayerNorm2d\n",
    "\n",
    "__all__ = [\"RDNet\"]\n",
    "\n",
    "\n",
    "class RDNetClassifierHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        num_classes: int,\n",
    "        drop_rate: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_features = in_features\n",
    "\n",
    "        self.norm = nn.LayerNorm(in_features)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def reset(self, num_classes):\n",
    "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, pre_logits: bool = False):\n",
    "        x = x.mean([-2, -1])\n",
    "        x = self.norm(x)\n",
    "        x = self.drop(x)\n",
    "        if pre_logits:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchifyStem(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_init_features, patch_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, num_init_features, kernel_size=patch_size, stride=patch_size),\n",
    "            LayerNorm2d(num_init_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stem(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
    "    def __init__(self, in_chs, inter_chs, out_chs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
    "            LayerNorm2d(in_chs, eps=1e-6),\n",
    "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class BlockESE(nn.Module):\n",
    "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
    "    def __init__(self, in_chs, inter_chs, out_chs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
    "            LayerNorm2d(in_chs, eps=1e-6),\n",
    "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
    "            EffectiveSEModule(out_chs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features,\n",
    "        growth_rate,\n",
    "        bottleneck_width_ratio,\n",
    "        drop_path_rate,\n",
    "        drop_rate=0.0,\n",
    "        rand_gather_step_prob=0.0,\n",
    "        block_idx=0,\n",
    "        block_type=\"Block\",\n",
    "        ls_init_value=1e-6,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.rand_gather_step_prob = rand_gather_step_prob\n",
    "        self.block_idx = block_idx\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate)) if ls_init_value > 0 else None\n",
    "        growth_rate = int(growth_rate)\n",
    "        inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8\n",
    "\n",
    "        if self.drop_path_rate > 0:\n",
    "            self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.layers = eval(block_type)(\n",
    "            in_chs=num_input_features,\n",
    "            inter_chs=inter_chs,\n",
    "            out_chs=growth_rate,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, List):\n",
    "            x = torch.cat(x, 1)\n",
    "        x = self.layers(x)\n",
    "\n",
    "        if self.gamma is not None:\n",
    "            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n",
    "\n",
    "        if self.drop_path_rate > 0 and self.training:\n",
    "            x = self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseStage(nn.Sequential):\n",
    "    def __init__(self, num_block, num_input_features, drop_path_rates, growth_rate, **kwargs):\n",
    "        super().__init__()\n",
    "        for i in range(num_block):\n",
    "            layer = DenseBlock(\n",
    "                num_input_features=num_input_features,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_path_rate=drop_path_rates[i],\n",
    "                block_idx=i,\n",
    "                **kwargs,\n",
    "            )\n",
    "            num_input_features += growth_rate\n",
    "            self.add_module(f\"dense_block{i}\", layer)\n",
    "        self.num_out_features = num_input_features\n",
    "\n",
    "    def forward(self, init_feature):\n",
    "        features = [init_feature]\n",
    "        for module in self:\n",
    "            new_feature = module(features)\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class RDNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_init_features=64,\n",
    "        growth_rates=(64, 104, 128, 128, 128, 128, 224),\n",
    "        num_blocks_list=(3, 3, 3, 3, 3, 3, 3),\n",
    "        bottleneck_width_ratio=4,\n",
    "        zero_head=False,\n",
    "        in_chans=3,  # timm option [--in-chans]\n",
    "        num_classes=1000,  # timm option [--num-classes]\n",
    "        drop_rate=0.0,  # timm option [--drop: dropout ratio]\n",
    "        drop_path_rate=0.0,  # timm option [--drop-path: drop-path ratio]\n",
    "        checkpoint_path=None,  # timm option [--initial-checkpoint]\n",
    "        transition_compression_ratio=0.5,\n",
    "        ls_init_value=1e-6,\n",
    "        is_downsample_block=(None, True, True, False, False, False, True),\n",
    "        block_type=\"Block\",\n",
    "        head_init_scale: float = 1.,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(growth_rates) == len(num_blocks_list) == len(is_downsample_block)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if isinstance(block_type, str):\n",
    "            block_type = [block_type] * len(growth_rates)\n",
    "\n",
    "        # stem\n",
    "        self.stem = PatchifyStem(in_chans, num_init_features, patch_size=4)\n",
    "\n",
    "        # features\n",
    "        self.feature_info = []\n",
    "        self.num_stages = len(growth_rates)\n",
    "        curr_stride = 4  # stem_stride\n",
    "        num_features = num_init_features\n",
    "        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(num_blocks_list)).split(num_blocks_list)]\n",
    "\n",
    "        dense_stages = []\n",
    "        for i in range(self.num_stages):\n",
    "            dense_stage_layers = []\n",
    "            if i != 0:\n",
    "                compressed_num_features = int(num_features * transition_compression_ratio / 8) * 8\n",
    "                k_size = stride = 1\n",
    "                if is_downsample_block[i]:\n",
    "                    curr_stride *= 2\n",
    "                    k_size = stride = 2\n",
    "                dense_stage_layers.append(LayerNorm2d(num_features))\n",
    "                dense_stage_layers.append(\n",
    "                    nn.Conv2d(num_features, compressed_num_features, kernel_size=k_size, stride=stride, padding=0)\n",
    "                )\n",
    "                num_features = compressed_num_features\n",
    "\n",
    "            stage = DenseStage(\n",
    "                num_block=num_blocks_list[i],\n",
    "                num_input_features=num_features,\n",
    "                growth_rate=growth_rates[i],\n",
    "                bottleneck_width_ratio=bottleneck_width_ratio,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rates=dp_rates[i],\n",
    "                ls_init_value=ls_init_value,\n",
    "                block_type=block_type[i],\n",
    "            )\n",
    "            dense_stage_layers.append(stage)\n",
    "            num_features += num_blocks_list[i] * growth_rates[i]\n",
    "\n",
    "            if i + 1 == self.num_stages or (i + 1 != self.num_stages and is_downsample_block[i + 1]):\n",
    "                self.feature_info += [\n",
    "                    dict(\n",
    "                        num_chs=num_features,\n",
    "                        reduction=curr_stride,\n",
    "                        module=f'dense_stages.{i}',\n",
    "                        growth_rate=growth_rates[i],\n",
    "                    )\n",
    "                ]\n",
    "            dense_stages.append(nn.Sequential(*dense_stage_layers))\n",
    "        self.dense_stages = nn.Sequential(*dense_stages)\n",
    "\n",
    "        # classifier\n",
    "        self.head = RDNetClassifierHead(num_features, num_classes, drop_rate=drop_rate)\n",
    "\n",
    "        # initialize weights\n",
    "        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n",
    "\n",
    "        if zero_head:\n",
    "            nn.init.zeros_(self.head[-1].weight.data)\n",
    "            if self.head[-1].bias is not None:\n",
    "                nn.init.zeros_(self.head[-1].bias.data)\n",
    "\n",
    "        if checkpoint_path is not None:\n",
    "            self.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head.fc\n",
    "\n",
    "    def reset_classifier(self, num_classes=0, global_pool=None):\n",
    "        assert global_pool is None\n",
    "        self.head.reset(num_classes)\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.dense_stages(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def group_matcher(self, coarse=False):\n",
    "        assert not coarse\n",
    "        return dict(\n",
    "            stem=r'^stem',\n",
    "            blocks=r'^dense_stages\\.(\\d+)',\n",
    "        )\n",
    "\n",
    "\n",
    "def _init_weights(module, name=None, head_init_scale=1.0):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        if name and 'head.' in name:\n",
    "            module.weight.data.mul_(head_init_scale)\n",
    "            module.bias.data.mul_(head_init_scale)\n",
    "\n",
    "\n",
    "def _create_rdnet(variant, pretrained=False, **kwargs):\n",
    "    if kwargs.get(\"pretrained_cfg\", \"\") == \"fcmae\":\n",
    "        # NOTE fcmae pretrained weights have no classifier or final norm-layer (`head.norm`)\n",
    "        # This is workaround loading with num_classes=0 w/o removing norm-layer.\n",
    "        kwargs.setdefault(\"pretrained_strict\", False)\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        RDNet, variant, pretrained, feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True), **kwargs\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"num_classes\": 1000,\n",
    "        \"input_size\": (3, 224, 224),\n",
    "        \"crop_pct\": 0.9,\n",
    "        \"interpolation\": \"bicubic\",\n",
    "        \"mean\": IMAGENET_DEFAULT_MEAN,\n",
    "        \"std\": IMAGENET_DEFAULT_STD,\n",
    "        \"first_conv\": \"stem.stem.0\",\n",
    "        \"classifier\": \"head.fc\",\n",
    "        **kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = generate_default_cfgs({\n",
    "    'rdnet_tiny.nv_in1k': _cfg(\n",
    "        hf_hub_id='naver-ai/rdnet_tiny.nv_in1k',\n",
    "    ),\n",
    "    'rdnet_small.nv_in1k': _cfg(\n",
    "        hf_hub_id='naver-ai/rdnet_small.nv_in1k',\n",
    "    ),\n",
    "    'rdnet_base.nv_in1k': _cfg(\n",
    "        hf_hub_id='naver-ai/rdnet_base.nv_in1k',\n",
    "    ),\n",
    "    'rdnet_large.nv_in1k': _cfg(\n",
    "        hf_hub_id='naver-ai/rdnet_large.nv_in1k',\n",
    "    ),\n",
    "    'rdnet_large.nv_in1k_ft_in1k_384': _cfg(\n",
    "        hf_hub_id='naver-ai/rdnet_large.nv_in1k_ft_in1k_384',\n",
    "        input_size=(3, 384, 384),\n",
    "        crop_pct=1.0,\n",
    "    ),\n",
    "})\n",
    "\n",
    "\n",
    "@register_model\n",
    "def rdnet_tiny(pretrained=False, **kwargs):\n",
    "    n_layer = 7\n",
    "    model_args = {\n",
    "        \"num_init_features\": 64,\n",
    "        \"growth_rates\": [64] + [104] + [128] * 4 + [224],\n",
    "        \"num_blocks_list\": [3] * n_layer,\n",
    "        \"is_downsample_block\": (None, True, True, False, False, False, True),\n",
    "        \"transition_compression_ratio\": 0.5,\n",
    "        \"block_type\": [\"Block\"] + [\"Block\"] + [\"BlockESE\"] * 4 + [\"BlockESE\"],\n",
    "    }\n",
    "    model = _create_rdnet(\"rdnet_tiny\", pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def rdnet_small(pretrained=False, **kwargs):\n",
    "    n_layer = 11\n",
    "    model_args = {\n",
    "        \"num_init_features\": 72,\n",
    "        \"growth_rates\": [64] + [128] + [128] * (n_layer - 4) + [240] * 2,\n",
    "        \"num_blocks_list\": [3] * n_layer,\n",
    "        \"is_downsample_block\": (None, True, True, False, False, False, False, False, False, True, False),\n",
    "        \"transition_compression_ratio\": 0.5,\n",
    "        \"block_type\": [\"Block\"] + [\"Block\"] + [\"BlockESE\"] * (n_layer - 4) + [\"BlockESE\"] * 2,\n",
    "    }\n",
    "    model = _create_rdnet(\"rdnet_small\", pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def rdnet_base(pretrained=False, **kwargs):\n",
    "    n_layer = 11\n",
    "    model_args = {\n",
    "        \"num_init_features\": 120,\n",
    "        \"growth_rates\": [96] + [128] + [168] * (n_layer - 4) + [336] * 2,\n",
    "        \"num_blocks_list\": [3] * n_layer,\n",
    "        \"is_downsample_block\": (None, True, True, False, False, False, False, False, False, True, False),\n",
    "        \"transition_compression_ratio\": 0.5,\n",
    "        \"block_type\": [\"Block\"] + [\"Block\"] + [\"BlockESE\"] * (n_layer - 4) + [\"BlockESE\"] * 2,\n",
    "    }\n",
    "    model = _create_rdnet(\"rdnet_base\", pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def rdnet_large(pretrained=False, **kwargs):\n",
    "    n_layer = 12\n",
    "    model_args = {\n",
    "        \"num_init_features\": 144,\n",
    "        \"growth_rates\": [128] + [192] + [256] * (n_layer - 4) + [360] * 2,\n",
    "        \"num_blocks_list\": [3] * n_layer,\n",
    "        \"is_downsample_block\": (None, True, True, False, False, False, False, False, False, False, True, False),\n",
    "        \"transition_compression_ratio\": 0.5,\n",
    "        \"block_type\": [\"Block\"] + [\"Block\"] + [\"BlockESE\"] * (n_layer - 4) + [\"BlockESE\"] * 2,\n",
    "    }\n",
    "    model = _create_rdnet(\"rdnet_large\", pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _create_rdnet(variant='rdnet_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 23513664\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "model\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23513664"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
