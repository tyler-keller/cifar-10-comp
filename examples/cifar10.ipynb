{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, datasets, callbacks, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR) # or logging.INFO, logging.WARNING, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DenseNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# rewrite of https://github.com/naver-ai/rdnet/blob/main/rdnet/rdnet.py in tensorflow\n",
    "\n",
    "# poor man's dense net\n",
    "# '''\n",
    "\n",
    "# class DenseNet:\n",
    "#     def __init__(self):\n",
    "#         # rdnet_tiny:\n",
    "#         # n_layer = 7\n",
    "#         # num_init_features: 64,\n",
    "#         # growth_rates: [64] + [104] + [128] * 4 + [224],\n",
    "#         # num_blocks_list: [3] * n_layer,\n",
    "#         # is_downsample_block: (None, True, True, False, False, False, True),\n",
    "#         # transition_compression_ratio: 0.5,\n",
    "\n",
    "#         input_shape = (32, 32, 3)\n",
    "\n",
    "#         self.class_num = 10\n",
    "\n",
    "#         self.n_layers = 5 \n",
    "#         self.num_init_features = 64 \n",
    "#         # self.growth_rates = [64] + [104] + [128] * 4 + [224]\n",
    "#         self.growth_rates = [32] + [64] + [96] * 2 + [128]\n",
    "#         self.is_downsample_block = (False, True, True, False, False, False, True)\n",
    "#         self.transition_compression_ratio = 0.5\n",
    "#         self.dropout_rate = 0.2\n",
    "\n",
    "#         self.epsilon = 1e-4\n",
    "\n",
    "#         self.model = self.build_network(input_shape)\n",
    "\n",
    "#         assert self.n_layers == len(self.growth_rates)\n",
    "\n",
    "#     def transition_layer(self, x, scope):\n",
    "#         '''compresses concat features'''\n",
    "#         x = layers.Conv2D(int(x.shape[-1] * self.transition_compression_ratio), kernel_size=2, strides=2, padding='same', use_bias=False, name=f'{scope}_conv1')(x)\n",
    "#         x = layers.LayerNormalization()(x)\n",
    "#         return x\n",
    "\n",
    "#     def dense_block(self, x, inter_channels, out_channels, scope, block_idx, layer_idx):\n",
    "#         x = layers.Conv2D(int(x.shape[-1]), kernel_size=7, strides=1, padding='same', activation='relu', use_bias=False, \n",
    "#                         name=f'{scope}_block{block_idx}_layer{layer_idx}_conv1')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Conv2D(inter_channels, kernel_size=1, strides=1, padding='same', activation='relu', use_bias=False, \n",
    "#                         name=f'{scope}_block{block_idx}_layer{layer_idx}_conv2')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Conv2D(out_channels, kernel_size=1, strides=1, padding='same', activation='relu', use_bias=False, \n",
    "#                         name=f'{scope}_block{block_idx}_layer{layer_idx}_conv3')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         return x\n",
    "\n",
    "#     def dense_stage(self, x, i, layer_name):\n",
    "#         layers_concat = [x]\n",
    "#         for j in range(3):  # loop over layers in the dense block\n",
    "#             dense_block = self.dense_block(\n",
    "#                 x, inter_channels=(self.growth_rates[i] * 4), out_channels=self.growth_rates[i], \n",
    "#                 scope=layer_name, block_idx=i, layer_idx=j\n",
    "#             )\n",
    "#             layers_concat.append(dense_block)\n",
    "#             x = layers.Concatenate(axis=-1, name=f'{layer_name}_concat_{j}')(layers_concat)\n",
    "#             x = layers.LayerNormalization()(x)\n",
    "#         return x\n",
    "\n",
    "#     def build_network(self, input_shape):\n",
    "#         inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "#         x = layers.Conv2D(self.num_init_features, kernel_size=7, strides=2, padding='same', activation='relu', use_bias=False, name='conv0')(inputs)\n",
    "#         x = layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         for i in range(self.n_layers - 1):\n",
    "#             x = self.dense_stage(x, i=i, layer_name=f'dense_{i + 1}')\n",
    "#             if i != 0 and self.is_downsample_block[i]:\n",
    "#                 x = self.transition_layer(x, scope=f'trans_{i + 1}')\n",
    "        \n",
    "#         x = self.dense_stage(x, i=-1, layer_name='dense_final')\n",
    "\n",
    "#         # x = layers.LayerNormalization(epsilon=self.epsilon, name='linear_batch')(x)\n",
    "#         # x = layers.Dropout(rate=self.dropout_rate)(x)\n",
    "#         x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "#         outputs = layers.Dense(units=self.class_num, activation='softmax')(x)\n",
    "#         return keras.Model(inputs, outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.optimizer.set_jit(False)  # disable xla\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = DenseNet().model\n",
    "#     # model = DenseNetSimple()\n",
    "\n",
    "#     print(model.summary())\n",
    "\n",
    "#     learning_rate = 0.001\n",
    "#     epochs = 300\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(\n",
    "#         learning_rate=learning_rate,\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#         train_dataset, \n",
    "#         epochs=epochs, \n",
    "#         validation_data=test_dataset,\n",
    "#         callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_labels = [train_label[0] for train_label in train_labels]\n",
    "test_labels = [test_label[0] for test_label in test_labels]\n",
    "train_labels = np.array(train_labels, dtype=np.int8)\n",
    "test_labels = np.array(test_labels, dtype=np.int8)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Model definition\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4), input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "with tf.device('/GPU:0'):\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return lr\n",
    "        elif lr <= 1e-4:\n",
    "            return 1e-4\n",
    "        else:\n",
    "            return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=[lr_scheduler, callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save('../models/simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CIFAR-10 dataset\n",
    "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "# train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "# train_labels = [train_label[0] for train_label in train_labels]\n",
    "# test_labels = [test_label[0] for test_label in test_labels]\n",
    "# train_labels = np.array(train_labels, dtype=np.int8)\n",
    "# test_labels = np.array(test_labels, dtype=np.int8)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "# test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Data augmentation\n",
    "# data_augmentation = tf.keras.Sequential([\n",
    "#     layers.RandomFlip(\"horizontal\"),\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "# ])\n",
    "\n",
    "# class ResidualBlock(layers.Layer):\n",
    "#     def __init__(self, filters, kernel_size=3, strides=1):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = layers.Conv2D(filters, kernel_size, strides=strides, padding=\"same\", use_bias=False)\n",
    "#         self.bn1 = layers.BatchNormalization()\n",
    "#         self.relu = layers.ReLU()\n",
    "#         self.conv2 = layers.Conv2D(filters, kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "#         self.bn2 = layers.BatchNormalization()\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = self.conv1(inputs)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         return self.relu(x + inputs)  # residual connection\n",
    "\n",
    "# def create_resnet_model():\n",
    "#     inputs = layers.Input(shape=(32, 32, 3))\n",
    "#     x = data_augmentation(inputs)\n",
    "#     x = layers.Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     for _ in range(3):  # stack residual blocks\n",
    "#         x = ResidualBlock(64)(x)\n",
    "\n",
    "#     x = layers.Conv2D(128, (3, 3), strides=2, padding=\"same\", use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.ReLU()(x)\n",
    "\n",
    "#     for _ in range(3):\n",
    "#         x = ResidualBlock(128)(x)\n",
    "\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# clr = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     first_decay_steps=2000,\n",
    "#     t_mul=2.0,\n",
    "#     m_mul=0.5,\n",
    "#     alpha=1e-5\n",
    "# )\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=clr)\n",
    "\n",
    "# # Compile the model\n",
    "# model = create_resnet_model()\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=test_dataset,\n",
    "#     epochs=50,\n",
    "#     callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "#     # verbose=1\n",
    "# )\n",
    "\n",
    "# model.save('../models/simple_resnet.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# IMAGE_SIZE = 32\n",
    "# PATCH_SIZE = 4\n",
    "# NUM_CLASSES = 10\n",
    "# EMBED_DIM = 64\n",
    "# NUM_HEADS = 4\n",
    "# NUM_LAYERS = 8\n",
    "# MLP_DIM = 128\n",
    "# DROPOUT_RATE = 0.1\n",
    "# BATCH_SIZE = 128\n",
    "# EPOCHS = 50\n",
    "\n",
    "# # Preprocess data\n",
    "# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "# train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "# train_labels = [train_label[0] for train_label in train_labels]\n",
    "# test_labels = [test_label[0] for test_label in test_labels]\n",
    "# train_labels = np.array(train_labels, dtype=np.int8)\n",
    "# test_labels = np.array(test_labels, dtype=np.int8)\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "# train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "# test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Vision Transformer Layers\n",
    "# class PatchEmbedding(layers.Layer):\n",
    "#     def __init__(self, patch_size, embed_dim):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.projection = layers.Conv2D(embed_dim, patch_size, patch_size, padding=\"valid\")\n",
    "#         self.flatten = layers.Reshape((-1, embed_dim))\n",
    "\n",
    "#     def call(self, x):\n",
    "#         patches = self.projection(x)\n",
    "#         return self.flatten(patches)\n",
    "\n",
    "# class TransformerBlock(layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate):\n",
    "#         super().__init__()\n",
    "#         self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.mlp = tf.keras.Sequential([\n",
    "#             layers.Dense(mlp_dim, activation=\"relu\"),\n",
    "#             layers.Dense(embed_dim),\n",
    "#         ])\n",
    "#         self.norm1 = layers.LayerNormalization()\n",
    "#         self.norm2 = layers.LayerNormalization()\n",
    "#         self.dropout1 = layers.Dropout(dropout_rate)\n",
    "#         self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         attn_output = self.attn(x, x)\n",
    "#         x = self.norm1(x + self.dropout1(attn_output))\n",
    "#         mlp_output = self.mlp(x)\n",
    "#         return self.norm2(x + self.dropout2(mlp_output))\n",
    "\n",
    "# class VisionTransformer(models.Model):\n",
    "#     def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, num_layers, mlp_dim, dropout_rate):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
    "#         self.cls_token = tf.Variable(tf.zeros((1, 1, embed_dim)), trainable=True)\n",
    "#         self.pos_embed = tf.Variable(tf.random.normal((1, (image_size // patch_size) ** 2 + 1, embed_dim)), trainable=True)\n",
    "#         self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)]\n",
    "#         self.mlp_head = tf.keras.Sequential([\n",
    "#             layers.LayerNormalization(),\n",
    "#             layers.Dense(num_classes, activation='softmax'),\n",
    "#         ])\n",
    "\n",
    "#     def call(self, x):\n",
    "#         batch_size = tf.shape(x)[0]\n",
    "#         x = self.patch_embed(x)\n",
    "#         cls_tokens = tf.broadcast_to(self.cls_token, (batch_size, 1, self.embed_dim))\n",
    "#         x = tf.concat([cls_tokens, x], axis=1)\n",
    "#         x += self.pos_embed\n",
    "#         for block in self.transformer_blocks:\n",
    "#             x = block(x)\n",
    "#         cls_output = x[:, 0]\n",
    "#         return self.mlp_head(cls_output)\n",
    "\n",
    "# # Build model\n",
    "# model = VisionTransformer(\n",
    "#     image_size=IMAGE_SIZE,\n",
    "#     patch_size=PATCH_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     embed_dim=EMBED_DIM,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     num_layers=NUM_LAYERS,\n",
    "#     mlp_dim=MLP_DIM,\n",
    "#     dropout_rate=DROPOUT_RATE,\n",
    "# )\n",
    "\n",
    "# clr = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "#     initial_learning_rate=1e-3,\n",
    "#     first_decay_steps=2000,\n",
    "#     t_mul=2.0,\n",
    "#     m_mul=0.5,\n",
    "#     alpha=1e-5\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=clr),\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# history = model.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=test_dataset,\n",
    "#     epochs=EPOCHS,\n",
    "#     callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "# )\n",
    "\n",
    "# model.save('../models/simple_vit.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1.75])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comptetion Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMPETITION\n",
    "\n",
    "# #########################################\n",
    "# # DO NOT MODIFY THIS SECTION\n",
    "\n",
    "# # load the competition data\n",
    "# # The data is in the numpy array format:\n",
    "# #   competition_images: (100,32,32,3) contains 100 images\n",
    "# #   competition_labels: (100,1) contains class lables (0 to 9)\n",
    "# import numpy as np\n",
    "# competition_data = np.load('../comp-template/competition_data.npz') \n",
    "# competition_images = competition_data['competition_images']\n",
    "# competition_labels = competition_data['competition_labels']\n",
    "\n",
    "\n",
    "# #########################################\n",
    "# # YOUR CODE/MODEL GOES HERE:\n",
    "\n",
    "# # load your model and/or trained weights\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras import models\n",
    "\n",
    "# my_model = models.load_model('../models/simple_cnn.keras')\n",
    "\n",
    "# config = my_model.get_config()\n",
    "# print(config)\n",
    "\n",
    "# my_model.save('../comp-template/simple_cnn.h5')\n",
    "\n",
    "# # # evaluate your model on the competition data\n",
    "# # # make any adjustment to the data format as needed to run your model\n",
    "# # # you must return accuracy of your model on the competition data \n",
    "# # competition_loss, competion_acc = my_model.evaluate(competition_images,  competition_labels)\n",
    "\n",
    "# # # MUST PRINT OUT THE ACCURACY OF YOUR MODEL ON THE COMPETITION DATA\n",
    "# # print('Accuracy:', competion_acc) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
